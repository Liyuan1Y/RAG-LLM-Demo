{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "YqrH0G07nYKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU bs4 tiktoken openai langchain pinecone-client[grpc] pypdf[full]"
      ],
      "metadata": {
        "id": "-nTHcHOgxoV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_folder_path = \"sample-location\" #clinical document location"
      ],
      "metadata": {
        "id": "ksVMdUysRY95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
        "dataset = loader.load()"
      ],
      "metadata": {
        "id": "8MMxrndhQTZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for doc in dataset:\n",
        "    data.append({\n",
        "        'reference': doc.metadata['source'].replace('rtdocs/', 'https://'),\n",
        "        'text': doc.page_content\n",
        "    })"
      ],
      "metadata": {
        "id": "2H2bsyVT0ClM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "nWXruhm5S5SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ],
      "metadata": {
        "id": "2FCYmaKGS87y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for idx, record in enumerate(tqdm(data)):\n",
        "    texts = text_splitter.split_text(record['text'])\n",
        "    chunks.extend([{\n",
        "        'id': str(uuid4()),\n",
        "        'text': texts[i],\n",
        "        'chunk': i,\n",
        "        'reference': record['reference']\n",
        "    } for i in range(len(texts))])"
      ],
      "metadata": {
        "id": "o-PQwiN1TXF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Model"
      ],
      "metadata": {
        "id": "JhaAONVNTmxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = \"\"  #OpenAI API Key\n",
        "\n",
        "embed_model = \"text-embedding-ada-002\""
      ],
      "metadata": {
        "id": "8pq7s6XcTkdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Storage"
      ],
      "metadata": {
        "id": "ZOpzgtL_nezR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "index_name = 'sample-vs'\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=\"\", #Pinecone API\n",
        "    environment=\"gcp-starter\"\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,\n",
        "        metric='cosine'\n",
        "    )\n",
        "\n",
        "index = pinecone.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "xcPrqVIiTql5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import datetime\n",
        "from time import sleep\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "    i_end = min(len(chunks), i+batch_size)\n",
        "    meta_batch = chunks[i:i_end]\n",
        "    ids_batch = [x['id'] for x in meta_batch]\n",
        "    texts = [x['text'] for x in meta_batch]\n",
        "    try:\n",
        "        res = client.embeddings.create(input=texts, model=embed_model)\n",
        "    except:\n",
        "        done = False\n",
        "        while not done:\n",
        "            sleep(5)\n",
        "            try:\n",
        "                res = client.embeddings.create(input=texts, model=embed_model)\n",
        "                done = True\n",
        "            except:\n",
        "                pass\n",
        "    embeds = [record.embedding for record in res.data]\n",
        "    meta_batch = [{\n",
        "        'text': x['text'],\n",
        "        'chunk': x['chunk'],\n",
        "        'reference': x['reference']\n",
        "    } for x in meta_batch]\n",
        "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
        "    index.upsert(vectors=to_upsert)"
      ],
      "metadata": {
        "id": "ATElJYPCT8Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval Agent"
      ],
      "metadata": {
        "id": "0qqJl4zhUKOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "index_name = 'sample-vs'\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=\"\",  #Pinecone API\n",
        "    environment=\"gcp-starter\"\n",
        ")\n",
        "\n",
        "index = pinecone.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "SCtzC1vGUIGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"\")\n",
        "\n",
        "query = str(\"\") #clinical query\n",
        "\n",
        "res = client.embeddings.create(\n",
        "    input=[query],\n",
        "    model=embed_model\n",
        ")\n",
        "\n",
        "xq = res.data[0].embedding\n",
        "res = index.query(xq, top_k=10, include_metadata=True)"
      ],
      "metadata": {
        "id": "4UmChHaJURTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Generation"
      ],
      "metadata": {
        "id": "M4nHGt-GUYAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contexts = [item['metadata']['text'] for item in res['matches']]\n",
        "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query"
      ],
      "metadata": {
        "id": "zZRL3nGNUYa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augmented_query)"
      ],
      "metadata": {
        "id": "dZGwGH1OUbGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Integration (GPT 4)"
      ],
      "metadata": {
        "id": "QQ4361Jum_rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"\"}, #System Prompt\n",
        "    {\"role\": \"user\", \"content\": augmented_query},\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "ouifbSEOnHpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "vIny4TvmnU4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}